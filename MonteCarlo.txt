\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{physics}
\usepackage{graphicx}
\usepackage[width=6.2in,height=8.4in]{geometry}
\usepackage{array}
\usepackage{subfigure}
\usepackage{multicol}
\usepackage{float}
\setlength{\parindent}{0em}
\usepackage{cancel}
\usepackage{mathptmx}
\usepackage{biblatex}
\addbibresource{references.bib}

\title{Monte Carlo Simulations\\ \Large{Florida International University}\\\large{Department of Physics}}
\author{Nestor Viana}
\date{September 2021} 

\begin{document}
\maketitle

\begin{abstract}
    Several Monte Carlo techniques are performed. These are used to determine the volume of the unit-radius $n$-dimensional sphere using acceptance/rejection method and to compare with values from a analytically derived formula of the $n$-dimensional sphere for $n=2,3,4,5$. We estimate the value of the mathematical constant $\pi = 3.142 \pm 0.001$ using the $n=2$ case to two decimal places of accuracy. Lastly, we build random number generators drawn from common Gaussian and Poisson probability distribution functions using the Box-Muller and transformation methods, respectively, and obtain accurate results through non-linear least-squares fitting and $\chi^2$ goodness-of-fit statistics. 
\end{abstract}
\medskip
\begin{multicols}{2}
\section{Introduction}

During World War II, a group of scientists and engineers at Alamogordo worked on building the first working electronic computer$-$the ENIAC. Almost upon completion, John Von Neumann, mathematics professor at the Institute for Advanced Study, sparked interest in using ENIAC for thermonuclear reactions, where him and other colleagues had a taste of its computational power. In 1946 in Los Alamos, a thorough review of this machine was presented, and among the audience stood Stan Ulam, professor of mathematics at the University of Southern California. Ulam had been weary that statistical tools for analysis were falling behind, so he discussed ideas for ENIAC with Von Neumann to prevent this field from becoming stagnant, and hence Monte Carlo emerged \cite{introMC}. 
Monte Carlo is a method built on the use of random numbers primarily utilized in computation of multiple integrals to simulate experimental calculations and estimate possible outcomes of theoretical models in fields such as physics, engineering, finance, risk analysis, forecasting, etc. A Monte Carlo simulation carries out a process (lab experiment) a thousand, million, or billion times, giving information about the distribution of parameters and variables and their uncertainty. This method is particularly useful when lab conditions aren't suitable for performing experiments that are difficult or virtually impossible to replicate usually due to lack of resources or other means, or for predicting behaviors of variables that may be otherwise unethical to test and measure in practice.  \hfill\break

The first part of this paper focuses in developing computational Monte Carlo techniques to obtain the volume of any $n$-dimensional sphere, thereby granting us access to measure the value of $\pi$ to virtually any degree of accuracy desired as long as it's within reasonable computational margin through the acceptance/rejection method. This method helps determine the probability density function of a variable given an \textit{auxiliary} random variable and its probability density function. The second half of this paper describes and utilizes the transformation methods used to generate random numerical deviates from Poisson and Gaussian distribution which implement in essence the same auxiliary mechanism of the acceptance/rejection method.

\section{Estimating $\pi$}\label{Estimatingpi}
\subsection{Method}
Consider a circle of radius 1 inscribed in a square of sidelength 2 centered at the origin (Figure 1). Let $(x,y)\in [-1,1]^2$ be a random point inside the square that may or may not be in the circle. The probability, $p$, that $(x,y)$ is inside the circle is proportional to the ratio of the area of the circle, $A_c$, to the area of the square, $A_s$. If $N_s$ is the total number of points that randomly lie in the square, then the (expected) number of points that also lie in the circle is 
\begin{equation}
    N_c = N_s\,p = N_s \frac{A_c}{A_s}.
\end{equation}
In practice, the area of the circle is unknown as is the value of $\pi$. The Monte Carlo procedure is as follows: obtain $N_s$ total points $\{(x_i,y_i)\}_{i=1}^{N_s}$, and if for any point $(x_i,y_i)$ the condition 
\begin{equation}
    x_i^2+y_i^2 \leq 1
\end{equation}
is true then $(x_i,y_i)$ lies inside the circle, and the value of $N_c$ increases by 1$-$this point is "accepted", otherwise it is "rejected" and $N_c$ remains constant. Thus, the value of $\pi$ is given by
\begin{align}
    A_s = \pi r^2 &= A_s\frac{N_c}{N_s}\\
            \pi &= 2^2\frac{N_c}{N_s}
\end{align}

%\includegraphics[scale=.7]{circlesquare.png}
\begin{figure}[H]
    \includegraphics[scale=.78]{circlesquare.png}
    \caption{Sketch of the Monte Carlo process.}
\end{figure}

Values for the points $\{(x,y)\}$ will be drawn from a uniform random number generator on the interval [0,1]. (We need not extend the interval to [-1,1] as both $x$ and $y$ shall be squared). 

\subsection{Error Analysis}
The number of points landing inside the circle $N_c$ is a discrete random variable. As described, $N_c$ models the number of "successes" of this Monte Carlo simulation, therefore it follows a Binomial Distribution with mean 
\begin{equation}
    \mu_{N_c} = N_s\, p
\end{equation}
and variance 
\begin{equation}
    \sigma_{N_c}^2 = N_s\,p(1-p) .
\end{equation}
Thus, from (4), the expectation value and standard deviation of our estimates are
\begin{equation}\label{mean in pi}
    \mu_\pi = 4\frac{N_c}{N_s}
\end{equation}
\begin{equation}\label{std in pi}
    \sigma_\pi = \frac{4}{N_s}\sqrt{N_c\qty(1-\frac{N_c}{N_s})}
\end{equation}

From \eqref{std in pi}, we can find the number of points $N_s$ needed to achieve any desired relative uncertainty, $\delta_\pi$, in our estimation of $\pi$. Using $p = \frac{\pi}{4}$ in \eqref{mean in pi}, 
\begin{align}
    \delta_\pi = \frac{\sigma_\pi}{\pi} &= \frac{4}{\pi N_s}\sqrt{N_c\qty(1-\frac{N_c}{N_s})}\\ &= \frac{4}{\pi N_s}\sqrt{N_s\frac{\pi}{4}\qty(1-\frac{\pi}{4})}
\end{align}
leads to 
\begin{equation}
    N_s = \qty(\frac{4}{\pi} - 1)\frac{1}{\delta_\pi^2}\approx \frac{0.273}{\delta_\pi^2}
\end{equation}
This means that for every one more wanted decimal point of relative uncertainty, a factor of $10^2$ more sample points is required.\hfill\break

If this Monte Carlo simulation is carried out $M$ times with $N_s$ sample points each, a number of $M$ estimates for $\pi$ will be produced. A convenient way to report these results, specially when $M$ is large, is a histogram plot of the estimate distribution $\mu_\pi$. In contrast to the variable $N_c$, the distribution of $\mu_\pi$ estimates behaves like a Normal Distribution as this variable is continuous and not discrete, and since its parent distribution $N_c$ is a Binomial one which converges to a Normal distribution as $M\to\infty$ by the Central Limit Theorem \cite{CLT}. It will be appropriate then to fit a Gaussian curve 
\begin{equation}\label{gaussianfit}
    G(x;\hat\mu,\hat\sigma, \hat A) \sim \hat A\exp\qty[-\frac{(x-\hat\mu)^2}{2\hat\sigma^2}]
\end{equation}
to the $\mu_\pi$ histogram plot (Figure 2). 

\subsubsection{Reduced $\chi^2$ Goodness-of-Fit Test}
The absolute estimate of $\pi$ and error reported will be the fit parameters $\hat\mu$ and $\hat\sigma$ of $G$ using a least-squares non-linear fit. To quantify the performance of this fit, a reduced $\displaystyle\chi^2$ goodness-of-fit test is carried out. Let $H_i$ be the content (frequency) of each histogram bin of the $\mu_\pi$ distribution plot with error $\sigma_i=\sqrt{H_i}$ and $x_i$ be the bin centers. The test statistic 
\begin{equation}
    \chi_\nu^2 = \frac{1}{\nu}\sum_{i=1}^M \frac{\qty[G\qty(x_i;\hat\mu,\hat\sigma, \hat A )-H_i]^2}{\sigma_i^2}
\end{equation}
is the ratio of the variance between the histogram data and the fitted curve to the variance of each histogram bin, where $\nu$ is the number of degrees of freedom. (In this case, $\nu=$ \#bins - \#parameters). One should see that if $\chi_\nu^2$ is too small or too big, then the variances between these two quantities at each $x_i$ do not correlate and therefore the fit is not satisfactory. Thus, a value of $\chi_\nu^2 \approx 1$ is most desired for good fit.

\subsection{Results}
We performed $M=1,000$ simulations with $N_s=10,000$ sample points each. A histogram of the results is plotted below together with a Gaussian fit described in \eqref{gaussianfit}. The fitted parameters and their uncertainties are reported as well.
\begin{align*}
    \hat\mu &= 3.1418 \pm 6\times 10^{-4}\\
    \hat\sigma &= 0.0170 \pm 6\times 10^{-4}\\
    \hat A &= 64.20 \pm 1.88 \\
    \chi_\nu^2 &= 0.862
\end{align*}

The parameter $\hat\mu$ is our estimate for the constant $\pi$:$$\pi = 3.142 \pm 0.001$$
The test statistic $\chi_\nu^2$ being quite close to unity tells us our Gaussian fit resembles the histogram data quite well, which is also evidenced by a close $\pi$ value.

\begin{figure}[H]\label{montecarlopihist}
    \includegraphics[scale=.57]{montecarlopi.png}
    \caption{Histogram plot and Gaussian fit of the distribution of the estimate of $\pi$, $\mu_\pi$.}
    \label{}
\end{figure}


\section{Volume of an $n$-Dimensional Sphere}

\subsection{Derivation}
\textbf{Proposition:} The measure (volume) of an $n$-dimensional sphere of radius $R$, $|V_n(R)|$, is proportional to the measure of the unit $n$-dimensional sphere by the relation
\begin{equation}
    |V_n(R)| = R^n |V_n(1)|
\end{equation}
\textbf{Proof:} The measure of an $n$-dimensional sphere of radius $R$ is given by 
\begin{equation}\label{nvolumex}
    |V_n(R)| = \int_{V_n(R)} dx_1 dx_2 \cdots dx_n 
\end{equation}
Make the change of variables $y_i = x_i/R$ to reduce the radius of the $n$-sphere to 1. 
\begin{figure}[H]
    \includegraphics[scale=.57]{balls.png}
    \caption{Change of variable transformation.}
    \label{}
\end{figure}
The Jacobian of this transformation is
\begin{align}
    \qty|\pdv{(x_1,\dots,x_n)}{(y_1,\dots,y_n)}|& =
    \begin{vmatrix}
        \displaystyle\pdv{x_1}{y_1} & \cdots & \displaystyle\pdv{x_1}{y_n} \\
        \vdots & \ddots & \vdots \\
        \displaystyle\pdv{x_n}{y_1} & \cdots & \displaystyle\pdv{x_n}{y_n}
    \end{vmatrix} \\
    &= \begin{vmatrix}
        R & \cdots & 0 \\
        \vdots & \ddots & \vdots \\
        0 & \cdots & R
    \end{vmatrix}
    = R^n
\end{align}
Hence \eqref{nvolumex} is equivalent to 
\begin{align}
    |V_n(R)| &= \int_{V_n(1)} R^n \,\,dy_1 dy_2 \cdots dy_n \\
            & = R^n \int_{V_n(1)} dy_1 dy_2 \cdots dy_n \\
            & = R^n |V_n(1)| \, . 
\end{align}

\begin{figure}[H]
    \centering
    \includegraphics[scale=1.1]{crosssection.png}
    \caption{Hypersphere cross-section. The variable $x_1$ is plotted against in one axis for simplicity; the vertical axis represents the axes of the rest of the variables. $A(x_1)$ is the ($n-1$)-hypersphere that results from the intersection.}
    \label{crosssection}
\end{figure}

\textbf{Theorem:}\label{volume formula} The measure of an $n$-sphere of radius $R$ is 
\begin{equation}
    |V_n(R)| = \frac{\pi^{n/2}}{\Gamma\qty(\frac{n}{2}+1)}R^n \qq{for} n\geq2.
\end{equation}

Before the proof of the theorem is presented, note that hyperplanes passing through the $n$-sphere are of dimension $n-1$. In particular, these hyperplane cross-sections are ($n-$1)-hyperspheres (Figure \ref{crosssection}), just like in 3 dimensions, planes crossing spheres form 2-dimensional circles, and how lines intersecting a 2-dimensional circle form 1-dimensional intervals. 

\textbf{Proof:} By the disk method, the volume of the unit $n$-hypersphere centered at the origin is the addition of the quantity $A(x_1)$ over $-1\leq x_1 \leq 1$. Moreover, $A(x_1)$ is the measure of the $(n-1)$-hypersphere at any $x_1$ value. 
\begin{align}
    |V_n(1)| &=\int_{-1}^1 A(x_1)\,dx_1 \\ 
            &= \int_{-1}^1 |V_{n-1}(r)|\,dx_1 \\
            &= \int_{-1}^1 \qty|V_{n-1}\qty(\sqrt{1-x_1^2})|\,dx_1 \\
\mqty{\text{by}\\\text{proposition}} &= \int_{-1}^1 \qty(\sqrt{1-x_1^2})^{n-1}|V_{n-1}(1)|\,dx_1 \\
            &= 2|V_{n-1}(1)|\int_0^1 (1-x^2)^{\frac{n-1}{2}}\,dx
\end{align}

Let $u = x^2$ so that
\begin{align}
    |V_n(1)| &= 2|V_{n-1}(1)|\int_0^1 (1-u)^{\frac{n-1}{2}}u^{-\frac12}\,du \\
             &= |V_{n-1}(1)|B\qty(\frac12,\frac{n-1}{2})
\end{align}
where $B$ is the Beta function 
\begin{equation}
    B(x,y)=\displaystyle\int_0^1 t^{x-1}(1-t)^{y-1}\,dt=\frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}
\end{equation}
and $\Gamma$ is the Gamma function. Substituting this recursive relation into (28) itself gives
\begin{align}
    |V_n(1)|&=|V_{n-1}(1)|\frac{\Gamma\qty(\frac12)\Gamma\qty(\frac{n+1}{2})}{\Gamma\qty(\frac{n+2}{2})}\\
    &= \sqrt\pi |V_{n-1}(1)| \frac{\Gamma\qty(\frac{n+1}{2})}{\Gamma\qty(\frac{n+2}{2})}\\
    &= \sqrt\pi \frac{\cancel{\Gamma\qty(\frac{n+1}{2})}}{\Gamma\qty(\frac{n+2}{2})}\sqrt\pi \frac{\cancel{\Gamma\qty(\frac{n}{2})}}{\cancel{\Gamma\qty(\frac{n+1}{2})}}\cdots \sqrt\pi \frac{\Gamma\qty(1)}{\cancel{\Gamma\qty(\frac{3}{2})}}\\ 
    &=\qty(\sqrt\pi)^n \frac{\Gamma(1)}{\Gamma\qty(\frac{n+2}{2})} = \frac{\pi^{n/2}}{\Gamma\qty(\frac{n}{2}+1)}.
\end{align}
From that last result and the previous proposition, 
\begin{equation}\label{eqofNdimsphere}
    \qty|V_n(R)| = \frac{\pi^{n/2}}{\Gamma\qty(\frac{n}{2}+1)} R^n \, .
\end{equation}

\subsection{Monte Carlo Method}
A similar Monte Carlo procedure is implemented as in the previous section; instead, we check the slightly different acceptance condition 
\begin{equation}
    x_1^2 + x_2^2 + \cdots + x_n^2 \leq 1
\end{equation}
is satisfied. That is, for any point $(x_1,x_2,...,x_n)\in [-1,1]^n$ generated randomly that satisfies Eq. 7 will lie inside the $n$-dimensional hypersphere inscribed in the $n$-dimensional hypercube of sidelength 2 centered at the origin. For each of these success points, the variable $N_c$ in \eqref{mcvolume} increases by 1. Values for the points $\{(x_1,x_2,...,x_n)\}$ are drawn from a uniform random number generator on the interval [0,1] for a total of $N_s$ points. 
\begin{equation}\label{mcvolume}
    |V_n(1)| = 2^n\frac{N_c}{N_s}
\end{equation}

\subsection{Error Analysis}
In this section we use a distinct approach to determine the measure of $V_n(1)$ and its uncertainty. For each run of this process, the standard deviation in $|V_n(1)|_i$ takes the form 
\begin{equation}
    \sigma_i = \frac{2^n}{N_s}\sqrt{N_c\qty(1-\frac{N_c}{N_s})}
\end{equation}

so that for a total of $M$ runs, we have $M$ estimates of $|V_n(1)|$. For large $M$, it's more convenient to summarize results with just a few statistics. Define the weighted mean and standard deviation of weighted mean respectively as
\begin{equation}\label{wmean}
    \mu = \frac{\displaystyle\sum\qty(|V_n(1)|_i/\sigma_i^2)}{\displaystyle\sum 1/\sigma_i^2}
\end{equation}
\begin{equation}\label{wstd}
    \sigma_\mu^2 = \sum 1/\sigma_i^2
\end{equation}

for each value of $n$.

\subsection{Results}
For each $n=2,\dots,5$ we perform $M=100$ trials with $N_s=10,000$ points each. Results of \eqref{wmean} and \eqref{wstd} are summarized in the table below together with the theoretical value of $|V_n(1)|$ from \eqref{eqofNdimsphere}.

\begin{table}[H]
\centering
\begin{tabular}{l|c|l}
\hline
$n$ & $\mu\pm\sigma_\mu$ & $|V_n(1)|$\\\hline
2 & 3.140 $\pm$ 0.001 & 3.1415 \\\hline
3 & 4.188 $\pm$ 0.004 & 4.1887 \\\hline
4 & 4.927 $\pm$ 0.007 & 4.9348 \\\hline
5 & 5.24  $\pm$ 0.01  & 5.2637\\\hline
\end{tabular}\caption{$n$-dimensional unit sphere volumes. The middle column are the results of the Monte Carlo simulation. Right column are the theoretical values.}
\end{table}

\section{Poisson Deviates}
\subsection{General Tranformation Method}\label{subsection4.1}
Numerical deviates are random numbers drawn from a particular probability distribution function $P(x)$. It is usual to start with a uniform probability distribution function $u(r)$ over the interval [0,1] and perform a transformation that allows us to obtain random numbers from $P(x)$. \cite{bevington}
\begin{equation}\label{uniformdist}
    u(r) = 
    \begin{cases}
    1 \qq{for} 0\leq r < 1\\
    0 \qq{everywhere else} 
    \end{cases}
\end{equation}

We would like to find a relation between $r$ and $x$ such that the probability measure of $u(r)$ and $P(x)$ are equal over a small region:
\begin{equation}
    |u(r)\,\Delta r| = |P(x)\,\Delta x|
\end{equation}
This is also known as conservation of probability. Since both $u$ and $P$ are nonnegative over all probability space we may remove the absolute values, and as both $\Delta r$ and $\Delta x\to 0,$
\begin{align}
    \int_{-\infty}^r u(r)\,dr &= \int_{-\infty}^x P(x)\,dx\\
    r &= \int_{-\infty}^x P(x)\,dx
\end{align}
Thus the corresponding deviate from the probability distribution $P$ is the upper bound of (43) that satisfies such equation. For a Poisson distribution function with mean $\mu$, we obtain deviates by finding integers $n$ such that for any random deviate $r$ from the distribution $u(r)$ the equation
\begin{equation}
    r \leq \sum_{k=0}^n \frac{\mu^k}{k!}e^{-\mu}
\end{equation}
is satisfied.
\subsection{Results}
Below we generate $N=1,000$ Poisson deviates using the process described in section \ref{subsection4.1} for each value of $\mu=1,10.3, 102.1$ and make histogram plots of the results.   
\begin{figure}[H]
    \centering
    \includegraphics[scale=.53]{PDmu=1p0.png}
    \caption{Poisson Monte Carlo deviates for $\mu=1$}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[scale=.53]{PDmu=10p3.png}
    \caption{Poisson Monte Carlo deviates for\qquad $\mu=10.3$; distribution behaves noticeably normal.}
\end{figure}

Note how the Central Limit Theorem begins to dictate the behavior of the distributions as $\mu$ gets larger. As the mean of the distributions strays away from 1, more deviates can generate to the left side of the histograms, making them adopt a Normal distribution shape. 
\begin{figure}[H]
    \centering
    \includegraphics[scale=.55]{PDmu=102p1.png}
    \caption{Poisson Monte Carlo deviates for\qquad $\mu=10.3$; distribution is essentially Gaussian.}
\end{figure}


\section{Gaussian Deviates}
\subsection{Box-Muller Transformation}
We reference the same method used in section \ref{subsection4.1} and (43). In this case, the probability distribution function is a Gaussian function with mean of 0 standard deviation of 1.
\begin{equation}\label{gaussian}
    P(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}\,.
\end{equation}
However, \eqref{gaussian} has no antiderivative in terms of elementary functions, so solving for the upper bound in (43) is tricky. The Box-Muller transformation method outlined in \cite{boxmuller} solves this issue. Let $r_1$ and $r_2$ be independently generated random numbers drawn from the unit square $(0,1)\times(0,1)$, meaning that any joint probability density function $f_{R_1,R_2}(r_1,r_2)$ of these two variables is equal to 1 over the unit square. Define 
\begin{align}
    z_1 &= \sqrt{-2\ln r_1}\cos(2\pi r_2) \\
    z_2 &= \sqrt{-2\ln r_1}\sin(2\pi r_2)
\end{align}
or equivalently, 
\begin{align}
    r_1 &= \exp\qty[-\frac12 (z_1^2+z_2^2)] \\
    r_2 &= \frac{1}{2\pi}\arctan\qty(\frac{z_2}{z_1})
\end{align}
We will show that the joint probability density function $f_{Z_1,Z_2}(z_1,z_2)$ is equal to the product of two functions $f_{Z_1}(z_1)f_{Z_2}(z_2)$ and that each $f_{Z_1}$  and $f_{Z_2}$ are Gaussian of mean 0 and variance 1. Using a change of variables,
\begin{equation}
    f_{Z_1,Z_2}(z_1,z_2) = f_{R_1,R_2}(z_1,z_2)\qty|\pdv{(r_1,r_2)}{(z_1,z_2)}|
\end{equation}
where 
\begin{align*}
    \qty|\pdv{(r_1,r_2)}{(z_1,z_2)}| &= \def\arraystretch{2}%%%%%%%%%%
    \begin{vmatrix}
        \displaystyle\pdv{r_1}{z_1} & \displaystyle\pdv{r_2}{z_1}\\
        \displaystyle\pdv{r_1}{z_2} & \displaystyle\pdv{r_2}{z_2}
    \end{vmatrix}\\
    &= \qty(\frac{1}{\sqrt{2\pi}}e^{-\displaystyle\frac{z_1^2}{2}}) \qty(\frac{1}{\sqrt{2\pi}}e^{-\displaystyle\frac{z_2^2}{2}})
\end{align*}
Since $f_{R_1,R_2}(z_1,z_2)=1$, it follows that the joint probability distribution function of $z_1$ and $z_2$ is a product of two independent functions, and each of these functions is a Gaussian. That is, $z_1$ and $z_2$ are Gaussian deviates.

\subsection{Results}
Results for the method discussed in previous section are presented in Figure 8. Additionally, we fit a Gaussian curve $G(x;\hat\mu,\hat\sigma,\hat A)$ to these data, which come from \eqref{gaussian} so the expected values for the mean and variance are $(\mu,\sigma)=(0,1)$. A similar method for fitting described in Section \ref{Estimatingpi} is utilized, including the reduced $\chi^2$ goodness-of-fit analysis.
\begin{align*}
    \hat\mu &= 0.0074\pm 0.0003\\
    \hat\sigma &= 0.099 \pm 0.002\\
    \hat A &= 1590 \pm 10  \\
    \chi_\nu^2 &= 0.993
\end{align*}
The test statistic $\chi_\nu^2$ being extremely close to 1 indicates that our Gaussian fit resembles the histogram data with little difference in variation between $G$ and histogram data values, which is in addition suggested by high accuracy of the fitted parameters $\hat\mu$ and $\hat\sigma$ to the expected quantities, respectively.

\begin{figure}[H]\label{gaussiandev}
    \centering
    \includegraphics[scale=.54]{GDmu=0sigma=1.png}
    \caption{Gaussian deviates from Box-Muller transformation, together fitted with a Normal distribution curve.}
    \label{fig:my_label}
\end{figure}

%\begin{thebibliography}{} 
%\bibitem{Bevington}“Monte Carlo Techniques.” Data Reduction and Error Analysis for the Physical Sciences, by Philip R. Bevington and D. Keith Robinson, McGraw-Hill, 2010. 
%\bibitem{CLT}https://ww3.Math.ucla.edu/, UCLA, www.stat.ucla.edu/~nchristo/introeconometrics/introecon_central_limit_theorem.pdf.
%\end{thebibliography}


\printbibliography

\end{multicols}

\end{document}
